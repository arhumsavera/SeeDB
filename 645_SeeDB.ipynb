{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeeDB Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An implementation of the sharing and pruning based optimizations in 'SEEDB: Efficient Data-Driven Visualization Recommendations to Support Visual Analytics'\n",
    "http://www.vldb.org/pvldb/vol8/p2182-vartak.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import psycopg2\n",
    "from scipy import stats as kl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from math import log,pi\n",
    "from decimal import Decimal\n",
    "\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.spatial.distance import euclidean, chebyshev, cityblock, jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hostname = 'localhost'\n",
    "username = 'arhumsavera'\n",
    "password = ''\n",
    "database = 'arhumsavera'\n",
    "\n",
    "continuous=['age','fnlwgt', 'education_num','capital_gain','capital_loss','hours_per_week']\n",
    "discrete=['workclass','education','occupation','relationship','race','sex','native_country','salary_range']\n",
    "aggregate=['sum','avg','max','min','count']\n",
    "\n",
    "k=10\n",
    "num_partitions=20\n",
    "delta= 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connection = psycopg2.connect( host=hostname, user=username, password=password, dbname=database )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_views():\n",
    "    views=[]\n",
    "    for f in aggregate:\n",
    "        for a in discrete:\n",
    "            for m in continuous:\n",
    "                views.append((a,m,f))\n",
    "    return views\n",
    "\n",
    "def get_query_result( conn, query ) :\n",
    "    cur = conn.cursor()\n",
    "    cur.execute( query )\n",
    "    return cur\n",
    "\n",
    "def normalize(f_list_1,f_list_2):\n",
    "    sum_1=sum(f_list_1)\n",
    "    sum_2=sum(f_list_2)\n",
    "    norm_1 = [i/sum_1 for i in f_list_1]\n",
    "    norm_2=[i/sum_2 for i in f_list_2]\n",
    "    return norm_1,norm_2\n",
    "\n",
    "def get_utility(f_list_1,f_list_2):\n",
    "    nf_1,nf_2=normalize(f_list_1,f_list_2)\n",
    "    nf_1=[float(x) for x in nf_1]\n",
    "    nf_2=[float(x) for x in nf_2]\n",
    "    return kl.entropy(nf_1,nf_2)\n",
    "\n",
    "\n",
    "def get_top_k(dict_kl, k):\n",
    "    return sorted(dict_kl.items(), key= lambda item: item[1], reverse=True)[:k]\n",
    "\n",
    "def get_iter(d):\n",
    "    if isinstance(d,dict):\n",
    "        return d.items()\n",
    "    else: #cursor\n",
    "        return d.fetchall()\n",
    "\n",
    "def get_agg_lists(o_1,o_2):\n",
    "    dict_res = defaultdict(list)\n",
    "    f_list_1 = []\n",
    "    f_list_2 = []\n",
    "    for k,v in get_iter(o_1):\n",
    "        if v != 0:\n",
    "            dict_res[k].append(v)\n",
    "        else:\n",
    "            dict_res[k].append(Decimal(1e-10))\n",
    "    for k,v in get_iter(o_2):\n",
    "        if k not in dict_res:\n",
    "            dict_res[k].append(Decimal(1e-10))\n",
    "        if v != 0:\n",
    "            dict_res[k].append(v)\n",
    "        else:\n",
    "            dict_res[k].append(Decimal(1e-10))\n",
    "    for k,v in dict_res.items():\n",
    "        if len(v) != 2:\n",
    "            dict_res[k].append(Decimal(1e-10))\n",
    "    for i in dict_res.values():\n",
    "        f_list_1.append(i[0])\n",
    "        f_list_2.append(i[1])\n",
    "    return f_list_1,f_list_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exhaustive Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('relationship', 'fnlwgt', 'sum'), [37.091433327244211]),\n",
       " (('relationship', 'capital_gain', 'sum'), [36.153433662242684]),\n",
       " (('relationship', 'capital_loss', 'sum'), [32.908131021036922]),\n",
       " (('relationship', 'hours_per_week', 'sum'), [30.798876735163201]),\n",
       " (('relationship', 'age', 'sum'), [29.595847300317548]),\n",
       " (('relationship', 'education_num', 'sum'), [29.430072938153241]),\n",
       " (('relationship', 'age', 'count'), [26.957476281616284]),\n",
       " (('relationship', 'fnlwgt', 'count'), [26.957476281616284]),\n",
       " (('relationship', 'education_num', 'count'), [26.957476281616284]),\n",
       " (('relationship', 'capital_gain', 'count'), [26.957476281616284])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_kl=defaultdict(list)\n",
    "dict_wd=defaultdict(list)\n",
    "dict_euclidean=defaultdict(list)\n",
    "dict_chebychev=defaultdict(list)\n",
    "dict_manhattan=defaultdict(list)\n",
    "dict_jaccard=defaultdict(list)\n",
    "\n",
    "\n",
    "views=get_all_views()\n",
    "for a,m,f in views:\n",
    "    #print (a,f,m)\n",
    "    q1='select '+a+', '+f+'('+m+') from married_adults where '+a+' is not null group by '+a+' order by '+a+';'\n",
    "    q2='select '+a+', '+f+'('+m+') from unmarried_adults where '+a+' is not null group by '+a+' order by '+a+';'\n",
    "    res_1=get_query_result(connection, q1)\n",
    "    res_2=get_query_result(connection, q2)\n",
    "    f_list_1,f_list_2 = get_agg_lists(res_1,res_2)\n",
    "    dict_kl[a,m,f].append(get_utility(f_list_1,f_list_2))\n",
    "\n",
    "    \n",
    "get_top_k(dict_kl, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharing-based Optimizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_combined_queries():\n",
    "    discrete_list=','.join(discrete)  # a,b,c\n",
    "    group_by_clause=' group by '+discrete_list  # group by a,b,c\n",
    "    \n",
    "    where_clause=' where '\n",
    "    for a in discrete:\n",
    "        where_clause+=' '+a+' is not null and'\n",
    "    where_clause=where_clause[:-3] # remove last and\n",
    "    \n",
    "    aggregate_continuous=''\n",
    "    for f in aggregate:\n",
    "        for m in continuous:\n",
    "            aggregate_continuous+= (','+f+'('+m+') as '+f+'_'+m)  #f1(m1) as f1_m1,f1(m2) as f1_m2..\n",
    "    \n",
    "    combined_queries=[]\n",
    "    for table_name in ['married','unmarried']:\n",
    "        combined_queries.append('select '+discrete_list+aggregate_continuous+' from '+table_name+'_adults'+where_clause+group_by_clause+' ;')\n",
    "    return combined_queries\n",
    "\n",
    "def apply_func(vals, func):\n",
    "    if func=='sum':\n",
    "        return sum(vals)\n",
    "    elif func=='max':\n",
    "        return max(vals)\n",
    "    elif func=='min':\n",
    "        return min(vals)\n",
    "    elif func=='count':\n",
    "        return sum(vals)\n",
    "\n",
    "def get_vals_avg(vals, counts):\n",
    "    sum_vals = 0\n",
    "    total_counts = 0\n",
    "    for a_i,c_i in zip(vals, counts):\n",
    "        sum_vals += a_i*c_i\n",
    "        total_counts += c_i\n",
    "    try:\n",
    "        return sum_vals/total_counts\n",
    "    except:\n",
    "        return Decimal(1e-10)\n",
    "    \n",
    "def partition_dataframe(num_partitions, df):\n",
    "    return np.array_split(df, num_partitions)\n",
    "\n",
    "\n",
    "def evaluate_dataframes(df_list,views):\n",
    "    dict_kl=defaultdict(list)\n",
    "    for a,m,f in views:\n",
    "        attr_list=[defaultdict(list), defaultdict(list)]\n",
    "        for i in range(len(df_list)):\n",
    "            for val in df_list[i][a].unique():  # for all unique values of an attribute\n",
    "                val_list=list(df_list[i][df_list[i][a] == val][\"{}_{}\".format(f,m)]) #get all corresponding vals\n",
    "                if f != 'avg':\n",
    "                    post_func_val=apply_func(val_list,f)  # apply function f\n",
    "                else:\n",
    "                    val_count_list=(list(df_list[i][df_list[i][a] == val][\"count_{}\".format(m)]))\n",
    "                    post_func_val=get_vals_avg(val_list,val_count_list)\n",
    "                attr_list[i][val]=post_func_val   # associate to dict\n",
    "\n",
    "        f_list_1,f_list_2 = get_agg_lists(attr_list[0],attr_list[1])\n",
    "        dict_kl[a,m,f].append(get_utility(f_list_1,f_list_2))\n",
    "    return dict_kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('relationship', 'capital_gain', 'sum'), [1.1158102467844995]),\n",
       " (('relationship', 'capital_loss', 'sum'), [1.051221189330178]),\n",
       " (('native_country', 'capital_loss', 'min'), [0.987729596647753]),\n",
       " (('relationship', 'hours_per_week', 'sum'), [0.9666960161238637]),\n",
       " (('relationship', 'education_num', 'sum'), [0.9517008867225196]),\n",
       " (('relationship', 'age', 'sum'), [0.9351947393994013]),\n",
       " (('relationship', 'age', 'count'), [0.9334707448570415]),\n",
       " (('relationship', 'fnlwgt', 'count'), [0.9334707448570415]),\n",
       " (('relationship', 'education_num', 'count'), [0.9334707448570415]),\n",
       " (('relationship', 'capital_gain', 'count'), [0.9334707448570415])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1,q2=get_combined_queries()  #married and unmarried \n",
    "\n",
    "res_1=get_query_result(connection, q1)\n",
    "res_2=get_query_result(connection, q2)\n",
    "\n",
    "df_res_1 = pd.DataFrame(res_1.fetchall())\n",
    "df_res_1.columns=[desc[0] for desc in res_1.description]\n",
    "df_res_2 = pd.DataFrame(res_2.fetchall())\n",
    "df_res_2.columns=[desc[0] for desc in res_2.description]\n",
    "\n",
    "dict_kl=defaultdict(list)\n",
    "views=get_all_views()\n",
    "df_list=[df_res_1, df_res_2]\n",
    "dict_kl=evaluate_dataframes_euclidean(df_list, views)\n",
    "\n",
    "get_top_k(dict_kl,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning & Sharing-Based Optimizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nf- normalized scores till now.\n",
    "# u- mean till now.\n",
    "# m - len(scores till now)\n",
    "# N- num partitions\n",
    "def get_confidence(m, N, delta):\n",
    "    t1=1-((m-1)/N)\n",
    "    t2=2*log(log(m))\n",
    "    t3=log(pi**2/(3*delta))\n",
    "    numerator=t1*t2+t3\n",
    "    denominator=2*m\n",
    "    epsilon_m=(numerator/denominator)**0.5\n",
    "    return epsilon_m\n",
    "\n",
    "def update_stats(dict_kl):\n",
    "    dict_stats=defaultdict(dict)\n",
    "    for key,vals in dict_kl.items():\n",
    "        dict_stats[key]['mean']=sum(vals)/len(vals)\n",
    "        CI = get_confidence(len(dict_kl[key]), num_partitions, delta)\n",
    "        dict_stats[key]['lower_CI'] = dict_stats[key]['mean'] - CI\n",
    "        dict_stats[key]['upper_CI'] = dict_stats[key]['mean'] + CI\n",
    "    return dict_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('relationship', 'fnlwgt', 'sum'), 34.507603720243523),\n",
       " (('relationship', 'capital_gain', 'sum'), 32.859483646589304),\n",
       " (('relationship', 'capital_loss', 'sum'), 30.185351370457795),\n",
       " (('relationship', 'hours_per_week', 'sum'), 28.077011516551885),\n",
       " (('relationship', 'age', 'sum'), 27.07816202815302),\n",
       " (('relationship', 'education_num', 'sum'), 26.739906038333782),\n",
       " (('relationship', 'capital_gain', 'max'), 26.222045853031187),\n",
       " (('native_country', 'capital_loss', 'avg'), 25.073083658266675),\n",
       " (('relationship', 'age', 'count'), 24.37680110414264),\n",
       " (('relationship', 'fnlwgt', 'count'), 24.37680110414264)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1,q2=get_combined_queries()  #married and unmarried \n",
    "\n",
    "res_1=get_query_result(connection, q1)\n",
    "res_2=get_query_result(connection, q2)\n",
    "\n",
    "df_res_1 = pd.DataFrame(res_1.fetchall())\n",
    "df_res_1.columns=[desc[0] for desc in res_1.description]\n",
    "df_res_2 = pd.DataFrame(res_2.fetchall())\n",
    "df_res_2.columns=[desc[0] for desc in res_2.description]\n",
    "\n",
    "\n",
    "\n",
    "df_res_1_partitions=partition_dataframe(num_partitions, df_res_1)\n",
    "df_res_2_partitions=partition_dataframe(num_partitions, df_res_2)\n",
    "\n",
    "dict_list_kl=defaultdict(list)\n",
    "dict_stats=defaultdict(dict)\n",
    "views= get_all_views()\n",
    "\n",
    "m=0\n",
    "for i in range(num_partitions):\n",
    "    df_list=[df_res_1_partitions[i],df_res_2_partitions[i]]\n",
    "    partition_dict_kl=evaluate_dataframes(df_list, views)\n",
    "    #add new partition KL values to already existing ones\n",
    "    for key, value in partition_dict_kl.items():\n",
    "        dict_list_kl[key].append(value[0])\n",
    "    m+=1\n",
    "    #update stats\n",
    "    if m>1: # math domain error for log(1) in confidence function\n",
    "        dict_stats=update_stats(dict_list_kl)  # key : {mean: _ , lower_CI:_ , upper_CI: }\n",
    "        #get all lower CI's\n",
    "        lower_CIs=[]\n",
    "        for triple,stats in dict_stats.items():\n",
    "            lower_CIs.append(stats['lower_CI'])\n",
    "        lower_CIs=sorted(lower_CIs, reverse=True)\n",
    "        k_lowest_CI=lower_CIs[k-1] # zero indexed\n",
    "        #remove all views whose keys have a lower upper CI than that\n",
    "        for view,stats in dict_stats.items():\n",
    "            if stats['upper_CI']<k_lowest_CI:\n",
    "                views.remove(view)\n",
    "                del dict_list_kl[view]\n",
    "    \n",
    "dict_kl=defaultdict(list)\n",
    "for key in dict_list_kl.keys():\n",
    "    dict_kl[key] = dict_stats[key]['mean']\n",
    "\n",
    "get_top_k(dict_kl, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Single Query Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#temp values for testing\n",
    "continuous=['age','capital_loss']\n",
    "discrete=['workclass','occupation']\n",
    "aggregate=['sum','avg','count']\n",
    "\n",
    "a='workclass'\n",
    "m='age'\n",
    "f='sum'\n",
    "q1='select '+a+', '+f+'('+m+') from married_adults where '+a+' is not null group by '+a+' order by '+a+' limit 20;'\n",
    "q2='select '+a+', '+f+'('+m+') from unmarried_adults where '+a+' is not null group by '+a+' order by '+a+' limit 20;'\n",
    "res_1=get_query_result(connection, q1)\n",
    "res_2=get_query_result(connection, q2)\n",
    "f_list_1 = []\n",
    "f_list_2 = []\n",
    "dict_res = defaultdict(list)\n",
    "for k,v in res_1.fetchall():\n",
    "    if v != 0:\n",
    "        dict_res[k].append(v)\n",
    "    else:\n",
    "        dict_res[k].append(Decimal(1e-10))\n",
    "for k,v in res_2.fetchall():\n",
    "    if k not in dict_res:\n",
    "        dict_res[k].append(Decimal(1e-10))\n",
    "    if v != 0:\n",
    "        dict_res[k].append(v)\n",
    "    else:\n",
    "        dict_res[k].append(Decimal(1e-10))\n",
    "for k,v in dict_res.items():\n",
    "    if len(v) != 2:\n",
    "        dict_res[k].append(Decimal(1e-10))\n",
    "for i in dict_res.values():\n",
    "    f_list_1.append(i[0])\n",
    "    f_list_2.append(i[1])\n",
    "print(f_list_1)\n",
    "print(f_list_2)\n",
    "nf_1,nf_2=normalize(f_list_1,f_list_2)\n",
    "nf_1=[float(x) for x in nf_1]\n",
    "nf_2=[float(x) for x in nf_2]\n",
    "kl.entropy(nf_1,nf_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
